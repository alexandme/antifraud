{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage and hash data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_value(value):\n",
    "    return hashlib.sha256(str(value).encode('utf-8')).hexdigest()\n",
    "\n",
    "columns_to_hash = ['funding_masked_card_number', 'payment_masked_card_number',\n",
    "                   'funding_card_bank_name', 'payment_card_bank_name']\n",
    "\n",
    "for column in columns_to_hash:\n",
    "    df[column] = df[column].apply(hash_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path and filename for the output CSV file\n",
    "output_file = '../data/findo_2022_06-08_h.csv'\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage hashed data on remote dev machine\n",
    "\n",
    "This code gets hashed Findo dataset, cleans it, gets around 100k samples, manages necessary columns transformations, etc. and saves dataset as S-FFSD.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sample_final_hashed_dataset.csv')\n",
    "\n",
    "# Drop column 'correlation_id.1'\n",
    "df = df.drop(columns=['correlation_id.1'])\n",
    "\n",
    "# Filter rows where 'status' is either 'APPROVED' or 'DECLINED'\n",
    "df = df[df['status'].isin(['APPROVED', 'DECLINED'])]\n",
    "\n",
    "# Add a new column 'labels' with default value 0\n",
    "df['labels'] = 0\n",
    "\n",
    "# Replace 'labels' values with '1' for currencies 'GHS', 'NGN', 'TTD', 'GMD','MGA' in the 'labels' column:\n",
    "df.loc[df['currency'].isin(['GHS', 'NGN', 'TTD', 'GMD', 'MGA']), 'labels'] = 1\n",
    "\n",
    "# Ensure the DataFrame is sorted by 'created_date'\n",
    "df.sort_values(by='created_date', inplace=True)\n",
    "\n",
    "# Add column 'time' and number rows in 'time' column starting from 0 \n",
    "df['time'] = range(len(df))\n",
    "\n",
    "# Leave only the columns: 'time', 'amount_usd', 'currency', 'labels', 'funding_masked_card_number', 'payment_masked_card_number', 'payment_card_country', 'payment_transaction_status'\n",
    "df = df[['time', 'amount_usd', 'labels', 'funding_masked_card_number', 'payment_masked_card_number', 'payment_card_country', 'status']]\n",
    "\n",
    "# Rename columns 'time' -> 'Time', 'funding_masked_card_number' -> 'Source', 'payment_masked_card_number' -> 'Target', 'amount_usd' -> 'Amount', 'labels' -> 'Labels',\n",
    "# 'payment_card_country' -> 'Location', 'status' -> 'Type'\n",
    "df = df.rename(columns={\n",
    "    'time': 'Time',\n",
    "    'funding_masked_card_number': 'Source',\n",
    "    'payment_masked_card_number': 'Target',\n",
    "    'amount_usd': 'Amount',\n",
    "    'labels': 'Labels',\n",
    "    'payment_card_country': 'Location',\n",
    "    'status': 'Type'\n",
    "})\n",
    "\n",
    "# Put columns in df in the following order: 'Time', 'Source', 'Target', 'Amount', 'Location', 'Type', 'Labels'\n",
    "df = df[['Time', 'Source', 'Target', 'Amount', 'Location', 'Type', 'Labels']]\n",
    "\n",
    "# Find the first and last row indices where 'Labels' equals 1\n",
    "first_index = df[df['Labels'] == 1].index[0]\n",
    "last_index = df[df['Labels'] == 1].index[-1]\n",
    "\n",
    "# Create a new dataframe containing all rows between these indices\n",
    "result_df = df.loc[first_index:last_index+1]\n",
    "\n",
    "# Create a mask for rows with 'Labels' equal to 0\n",
    "mask = result_df['Labels'] == 0\n",
    "\n",
    "# Get the indices of 20% of the rows with 'Labels' equal to 0\n",
    "indices_to_replace = result_df[mask].sample(frac=0.2).index\n",
    "\n",
    "# Replace the 'Labels' values at the selected indices with 2\n",
    "result_df.loc[indices_to_replace, 'Labels'] = 2\n",
    "\n",
    "# Replace values in 'Time' column: fill with numbers starting from 0 to the amount of rows\n",
    "result_df['Time'] = range(len(result_df))\n",
    "\n",
    "# Save dataframe to csv\n",
    "result_df.to_csv('../data/S-FFSD.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code loads the dataset and ensures that all columns and all data is in proper shape\n",
    "\n",
    "# Load data/S-FFSD.csv to df (This is Findo data sampled from the whole dataset and prepared for transformation to be used by training script)\n",
    "df = pd.read_csv('../data/S-FFSD.csv')\n",
    "\n",
    "# Show the amount of rows with 'Labels' 0, 1 and 2\n",
    "print(\"Amount of rows with 'Labels' 0:\", df[df['Labels'] == 0].shape[0])\n",
    "print(\"Amount of rows with 'Labels' 1:\", df[df['Labels'] == 1].shape[0])\n",
    "print(\"Amount of rows with 'Labels' 2:\", df[df['Labels'] == 2].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and methods\n",
    "\n",
    "This to experiment in notebook before running the graph training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import dgl\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATADIR = \"../data/\"\n",
    "\n",
    "def featmap_gen(tmp_df=None):\n",
    "    \"\"\"\n",
    "    Handle S-FFSD dataset and do some feature engineering\n",
    "    :param tmp_df: the feature of input dataset\n",
    "    \"\"\"\n",
    "    # time_span = [2, 5, 12, 20, 60, 120, 300, 600, 1500, 3600, 10800, 32400, 64800, 129600,\n",
    "    #              259200]  # Increase in the number of time windows to increase the characteristics.\n",
    "    time_span = [2, 3, 5, 15, 20, 50, 100, 150,\n",
    "                 200, 300, 864, 2590, 5100, 10000, 24000]\n",
    "    time_name = [str(i) for i in time_span]\n",
    "    time_list = tmp_df['Time']\n",
    "    post_fe = []\n",
    "    for trans_idx, trans_feat in tqdm(tmp_df.iterrows()):\n",
    "        new_df = pd.Series(trans_feat)\n",
    "        temp_time = new_df.Time\n",
    "        temp_amt = new_df.Amount\n",
    "        for length, tname in zip(time_span, time_name):\n",
    "            lowbound = (time_list >= temp_time - length)\n",
    "            upbound = (time_list <= temp_time)\n",
    "            correct_data = tmp_df[lowbound & upbound]\n",
    "            new_df['trans_at_avg_{}'.format(\n",
    "                tname)] = correct_data['Amount'].mean()\n",
    "            new_df['trans_at_totl_{}'.format(\n",
    "                tname)] = correct_data['Amount'].sum()\n",
    "            new_df['trans_at_std_{}'.format(\n",
    "                tname)] = correct_data['Amount'].std()\n",
    "            new_df['trans_at_bias_{}'.format(\n",
    "                tname)] = temp_amt - correct_data['Amount'].mean()\n",
    "            new_df['trans_at_num_{}'.format(tname)] = len(correct_data)\n",
    "            new_df['trans_target_num_{}'.format(tname)] = len(\n",
    "                correct_data.Target.unique())\n",
    "            new_df['trans_location_num_{}'.format(tname)] = len(\n",
    "                correct_data.Location.unique())\n",
    "            new_df['trans_type_num_{}'.format(tname)] = len(\n",
    "                correct_data.Type.unique())\n",
    "        post_fe.append(new_df)\n",
    "    return pd.DataFrame(post_fe)\n",
    "\n",
    "\n",
    "def sparse_to_adjlist(sp_matrix, filename):\n",
    "    \"\"\"\n",
    "    Transfer sparse matrix to adjacency list\n",
    "    :param sp_matrix: the sparse matrix\n",
    "    :param filename: the filename of adjlist\n",
    "    \"\"\"\n",
    "    # add self loop\n",
    "    homo_adj = sp_matrix + sp.eye(sp_matrix.shape[0])\n",
    "    # create adj_list\n",
    "    adj_lists = defaultdict(set)\n",
    "    edges = homo_adj.nonzero()\n",
    "    for index, node in enumerate(edges[0]):\n",
    "        adj_lists[node].add(edges[1][index])\n",
    "        adj_lists[edges[1][index]].add(node)\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(adj_lists, file)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def MinMaxScaling(data):\n",
    "    mind, maxd = data.min(), data.max()\n",
    "    # return mind + (data - mind) / (maxd - mind)\n",
    "    return (data - mind) / (maxd - mind)\n",
    "\n",
    "\n",
    "def k_neighs(\n",
    "    graph: dgl.DGLGraph,\n",
    "    center_idx: int,\n",
    "    k: int,\n",
    "    where: str,\n",
    "    choose_risk: bool = False,\n",
    "    risk_label: int = 1\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"return indices of risk k-hop neighbors\n",
    "\n",
    "    Args:\n",
    "        graph (dgl.DGLGraph): dgl graph dataset\n",
    "        center_idx (int): center node idx\n",
    "        k (int): k-hop neighs\n",
    "        where (str): {\"predecessor\", \"successor\"}\n",
    "        risk_label (int, optional): value of fruad label. Defaults to 1.\n",
    "    \"\"\"\n",
    "    target_idxs: torch.Tensor\n",
    "    if k == 1:\n",
    "        if where == \"in\":\n",
    "            neigh_idxs = graph.predecessors(center_idx)\n",
    "        elif where == \"out\":\n",
    "            neigh_idxs = graph.successors(center_idx)\n",
    "\n",
    "    elif k == 2:\n",
    "        if where == \"in\":\n",
    "            subg_in = dgl.khop_in_subgraph(\n",
    "                graph, center_idx, 2, store_ids=True)[0]\n",
    "            neigh_idxs = subg_in.ndata[dgl.NID][subg_in.ndata[dgl.NID] != center_idx]\n",
    "            # delete center node itself\n",
    "            neigh1s = graph.predecessors(center_idx)\n",
    "            neigh_idxs = neigh_idxs[~torch.isin(neigh_idxs, neigh1s)]\n",
    "        elif where == \"out\":\n",
    "            subg_out = dgl.khop_out_subgraph(\n",
    "                graph, center_idx, 2, store_ids=True)[0]\n",
    "            neigh_idxs = subg_out.ndata[dgl.NID][subg_in.ndata[dgl.NID] != center_idx]\n",
    "            neigh1s = graph.successors(center_idx)\n",
    "            neigh_idxs = neigh_idxs[~torch.isin(neigh_idxs, neigh1s)]\n",
    "\n",
    "    neigh_labels = graph.ndata['label'][neigh_idxs]\n",
    "    if choose_risk:\n",
    "        target_idxs = neigh_idxs[neigh_labels == risk_label]\n",
    "    else:\n",
    "        target_idxs = neigh_idxs\n",
    "\n",
    "    return target_idxs\n",
    "\n",
    "\n",
    "def count_risk_neighs(\n",
    "    graph: dgl.DGLGraph,\n",
    "    risk_label: int = 1\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    ret = []\n",
    "    for center_idx in graph.nodes():\n",
    "        neigh_idxs = graph.successors(center_idx)\n",
    "        neigh_labels = graph.ndata['label'][neigh_idxs]\n",
    "        risk_neigh_num = (neigh_labels == risk_label).sum()\n",
    "        ret.append(risk_neigh_num)\n",
    "\n",
    "    return torch.Tensor(ret)\n",
    "\n",
    "\n",
    "def feat_map():\n",
    "    tensor_list = []\n",
    "    feat_names = []\n",
    "    for idx in tqdm(range(graph.num_nodes())):\n",
    "        neighs_1_of_center = k_neighs(graph, idx, 1, \"in\")\n",
    "        neighs_2_of_center = k_neighs(graph, idx, 2, \"in\")\n",
    "\n",
    "        tensor = torch.FloatTensor([\n",
    "            edge_feat[neighs_1_of_center, 0].sum().item(),\n",
    "            # edge_feat[neighs_1_of_center, 0].std().item(),\n",
    "            edge_feat[neighs_2_of_center, 0].sum().item(),\n",
    "            # edge_feat[neighs_2_of_center, 0].std().item(),\n",
    "            edge_feat[neighs_1_of_center, 1].sum().item(),\n",
    "            # edge_feat[neighs_1_of_center, 1].std().item(),\n",
    "            edge_feat[neighs_2_of_center, 1].sum().item(),\n",
    "            # edge_feat[neighs_2_of_center, 1].std().item(),\n",
    "        ])\n",
    "        tensor_list.append(tensor)\n",
    "\n",
    "    feat_names = [\"1hop_degree\", \"2hop_degree\",\n",
    "                  \"1hop_riskstat\", \"2hop_riskstat\"]\n",
    "\n",
    "    tensor_list = torch.stack(tensor_list)\n",
    "    return tensor_list, feat_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Findo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "# \"\"\"\n",
    "#     For S-FFSD dataset\n",
    "# \"\"\"\n",
    "print(f\"processing Findo data...\")\n",
    "data = pd.read_csv(os.path.join(DATADIR, 'findo_2022_06-08_h_fe.csv'))\n",
    "data = featmap_gen(data.reset_index(drop=True))\n",
    "data.replace(np.nan, 0, inplace=True)\n",
    "data.to_csv(os.path.join(DATADIR, 'Findoneofull.csv'), index=None)\n",
    "data = pd.read_csv(os.path.join(DATADIR, 'Findoneofull.csv'))\n",
    "\n",
    "data = data.reset_index(drop=True)\n",
    "out = []\n",
    "alls = []\n",
    "allt = []\n",
    "pair = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "for column in pair:\n",
    "    src, tgt = [], []\n",
    "    edge_per_trans = 3\n",
    "    for c_id, c_df in tqdm(data.groupby(column), desc=column):\n",
    "        c_df = c_df.sort_values(by=\"Time\")\n",
    "        df_len = len(c_df)\n",
    "        sorted_idxs = c_df.index\n",
    "        src.extend([sorted_idxs[i] for i in range(df_len)\n",
    "                    for j in range(edge_per_trans) if i + j < df_len])\n",
    "        tgt.extend([sorted_idxs[i+j] for i in range(df_len)\n",
    "                    for j in range(edge_per_trans) if i + j < df_len])\n",
    "    alls.extend(src)\n",
    "    allt.extend(tgt)\n",
    "alls = np.array(alls)\n",
    "allt = np.array(allt)\n",
    "g = dgl.graph((alls, allt))\n",
    "cal_list = [\"Source\", \"Target\", \"Location\", \"Type\"]\n",
    "for col in cal_list:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = le.fit_transform(data[col].apply(str).values)\n",
    "feat_data = data.drop(\"Labels\", axis=1)\n",
    "labels = data[\"Labels\"]\n",
    "g.ndata['label'] = torch.from_numpy(\n",
    "    labels.to_numpy()).to(torch.long)\n",
    "g.ndata['feat'] = torch.from_numpy(\n",
    "    feat_data.to_numpy()).to(torch.float32)\n",
    "dgl.data.utils.save_graphs(DATADIR + \"graph-Findo.bin\", [g])\n",
    "\n",
    "# generate neighbor riskstat features\n",
    "for file_name in ['Findo']:\n",
    "    print(\n",
    "        f\"Generating neighbor risk-aware features for {file_name} dataset...\")\n",
    "    graph = dgl.load_graphs(DATADIR + \"graph-\" + file_name + \".bin\")[0][0]\n",
    "    graph: dgl.DGLGraph\n",
    "    print(f\"graph info: {graph}\")\n",
    "\n",
    "    edge_feat: torch.Tensor\n",
    "    degree_feat = graph.in_degrees().unsqueeze_(1).float()\n",
    "    risk_feat = count_risk_neighs(graph).unsqueeze_(1).float()\n",
    "\n",
    "    origin_feat_name = []\n",
    "    edge_feat = torch.cat([degree_feat, risk_feat], dim=1)\n",
    "    origin_feat_name = ['degree', 'riskstat']\n",
    "\n",
    "    features_neigh, feat_names = feat_map()\n",
    "    # print(f\"feature neigh: {features_neigh.shape}\")\n",
    "\n",
    "    features_neigh = torch.cat(\n",
    "        (edge_feat, features_neigh), dim=1\n",
    "    ).numpy()\n",
    "    feat_names = origin_feat_name + feat_names\n",
    "    features_neigh[np.isnan(features_neigh)] = 0.\n",
    "\n",
    "    output_path = DATADIR + file_name + \"_neigh_feat.csv\"\n",
    "    features_neigh = pd.DataFrame(features_neigh, columns=feat_names)\n",
    "    scaler = StandardScaler()\n",
    "    # features_neigh = np.log(features_neigh + 1)\n",
    "    features_neigh = pd.DataFrame(scaler.fit_transform(\n",
    "        features_neigh), columns=features_neigh.columns)\n",
    "\n",
    "    features_neigh.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features correlation analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns to prepare for correlation analisis\n",
    "columns_to_drop = ['created_date', 'correlation_id', 'mcc', 'transaction_type', 'payment_masked_card_number', 'funding_masked_card_number']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "df['payment_transaction_status'] = df['payment_transaction_status'].fillna('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numerical values if necessary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categorical_columns = ['class', 'status', 'currency', 'funding_provider', 'payment_provider', 'payment_card_country', 'funding_card_country', 'funding_card_bank_name', 'payment_transaction_status', 'payment_card_bank_name', 'acq_response_code', 'payment_transaction_currency']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    df[column] = label_encoder.fit_transform(df[column].astype(str))\n",
    "\n",
    "df = df.fillna(0)\n",
    "df['amount_usd'] = pd.to_numeric(df['amount_usd'], errors='coerce')\n",
    "df = df.reset_index(drop=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique currency codes or names : \", df['currency'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Correlation Heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (antifraud)",
   "language": "python",
   "name": "antifraud"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
